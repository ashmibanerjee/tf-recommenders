{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashmibanerjee/tf-recommenders/blob/main/notebook/TF%20Recommenders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Defining the `imports` and `utilities`"
      ],
      "metadata": {
        "id": "pZLxnM8ZZROG"
      },
      "id": "pZLxnM8ZZROG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb55XtSW-G8j",
        "outputId": "92f488de-4881-4b07-eb98-06c18b5a6e9e"
      },
      "id": "yb55XtSW-G8j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_recommenders in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_recommenders) (1.4.0)\n",
            "Requirement already satisfied: tensorflow>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_recommenders) (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.0->tensorflow_recommenders) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.9.0->tensorflow_recommenders) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.9.0->tensorflow_recommenders) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.9.0->tensorflow_recommenders) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.9.0->tensorflow_recommenders) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd8c88d",
      "metadata": {
        "id": "ccd8c88d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to pretty print the data\n",
        "def pprint_data(data: tf.data.Dataset, n: int=1):\n",
        "   for x in data.take(n).as_numpy_iterator():\n",
        "    pprint.pprint(x) "
      ],
      "metadata": {
        "id": "Ul5bI6QHXVS-"
      },
      "id": "Ul5bI6QHXVS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Preparing the Data\n"
      ],
      "metadata": {
        "id": "4XFIIhlxZL80"
      },
      "id": "4XFIIhlxZL80"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1: Download the data from `tensorflow_datasets`"
      ],
      "metadata": {
        "id": "FtO5aCnAcWMj"
      },
      "id": "FtO5aCnAcWMj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80872f53",
      "metadata": {
        "id": "80872f53"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ratings:\")\n",
        "pprint_data(ratings)\n",
        "print(\"Movies\")\n",
        "pprint_data(movies)"
      ],
      "metadata": {
        "id": "JYYQ5m_RXpw4"
      },
      "id": "JYYQ5m_RXpw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.2: Clean up the data\n",
        "\n",
        "In this example, we're going to focus on the ratings data.\n",
        "\n",
        "Hence, we keep only the `user_id`, and movie_title fields in the dataset."
      ],
      "metadata": {
        "id": "M75p4DfSZzNy"
      },
      "id": "M75p4DfSZzNy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2108ef7",
      "metadata": {
        "id": "b2108ef7"
      },
      "outputs": [],
      "source": [
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n",
        "\n",
        "pprint_data(ratings)\n",
        "pprint_data(movies)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.3: Train-Test Split\n",
        "\n",
        "To fit and evaluate the model, we need to split it into a training and evaluation set. In an industrial recommender system, this would most likely be done by time: the data up to time  ð‘‡  would be used to predict interactions after  ð‘‡ .\n",
        "\n",
        "In this simple example, however, let's use a random split, putting 80% of the ratings in the train set, and 20% in the test set."
      ],
      "metadata": {
        "id": "zwR2-Pc9cgg9"
      },
      "id": "zwR2-Pc9cgg9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ca5e0e",
      "metadata": {
        "id": "20ca5e0e"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "pprint_data(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.4: Unique` user_ids `and `movie_titles`\n",
        "\n",
        "* We figure out unique user ids and movie titles present in the data.\n",
        "\n",
        "* We batch the movies and ratings dataset in batches of 1000 and 1000000 movie titles and ratings respectively.\n",
        "\n",
        "* Batching allows us to process multiple movie titles simultaneously, which can improve computational efficiency during training.\n",
        "\n",
        "* This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. \n",
        "* To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
      ],
      "metadata": {
        "id": "Cf5bD8F0dL0n"
      },
      "id": "Cf5bD8F0dL0n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc92768e",
      "metadata": {
        "id": "dc92768e"
      },
      "outputs": [],
      "source": [
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "unique_movie_titles[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Model Implementation\n",
        "\n",
        "Since we are building a two-tower retrieval model, we can build each tower (the `Query Tower`/`User Embeddings` and the `Candidate Tower`/`Item Embeddings`) separately and then combine them in the final model."
      ],
      "metadata": {
        "id": "QTVEE5hPeHTE"
      },
      "id": "QTVEE5hPeHTE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.1: The Query Tower\n",
        "\n",
        "* Define the `dimensionality`\n",
        " \n",
        " The first step is to decide on the dimensionality of the query and candidate representations. \n",
        "\n",
        " *Higher values will correspond to models that may be more accurate, but will also be slower to fit and more prone to overfitting.*\n"
      ],
      "metadata": {
        "id": "pyFGYOO5gjb0"
      },
      "id": "pyFGYOO5gjb0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f475ca",
      "metadata": {
        "id": "83f475ca"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Defining the `user_model`\n",
        "\n",
        " Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those to user embeddings via an Embedding layer. \n",
        " \n",
        " * Note that we use the list of unique user ids we computed earlier as a vocabulary.\n",
        "\n",
        "This model corresponds to the **classic matrix factorization** approach."
      ],
      "metadata": {
        "id": "Otup5uPQhxM8"
      },
      "id": "Otup5uPQhxM8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69aabf6",
      "metadata": {
        "id": "b69aabf6"
      },
      "outputs": [],
      "source": [
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.2: The Candidate Tower\n",
        "\n",
        "Similarly, we build the `candidate tower`/`item embeddings`."
      ],
      "metadata": {
        "id": "7fEDdvIeiJ7R"
      },
      "id": "7fEDdvIeiJ7R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65848ebe",
      "metadata": {
        "id": "65848ebe"
      },
      "outputs": [],
      "source": [
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.3: Defining the `Metrics`\n",
        "\n",
        "In our training data, we have positive pairs of (user, movie) instances. To evaluate the performance of our model, we need to compare the affinity score generated by the model for each positive pair with the scores of all other potential candidates. If the score for the positive pair is higher than the scores of all other candidates, it indicates that our model is highly accurate.\n",
        "\n",
        "To accomplish this evaluation, we can utilize the tfrs.metrics.FactorizedTopK metric. This metric requires one essential argument: the dataset of candidates that will be considered as implicit negatives for evaluation.\n",
        "\n",
        "In our specific scenario, the dataset of candidates corresponds to the movies dataset. To facilitate the evaluation process, we convert the movie dataset into embeddings using our movie model. These embeddings capture the latent representations of the movies and enable us to compare them with the positive pairs during evaluation."
      ],
      "metadata": {
        "id": "MfYHImoxiWyG"
      },
      "id": "MfYHImoxiWyG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13668024",
      "metadata": {
        "id": "13668024"
      },
      "outputs": [],
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=movies.batch(128).map(movie_model)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.4: Defining the `Loss` function\n",
        "\n",
        "TFRS has several loss layers and tasks to make this easy.\n",
        "\n",
        "In this instance, we'll make use of the Retrieval task object: a convenience wrapper that bundles together the loss function and metric computation."
      ],
      "metadata": {
        "id": "KMks3rQkkz-n"
      },
      "id": "KMks3rQkkz-n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15a58d7",
      "metadata": {
        "id": "a15a58d7"
      },
      "outputs": [],
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.5: Combining the Models: The Full Model\n",
        "\n",
        "* Now we can combine all the components into a complete model. \n",
        "* TensorFlow Recommenders (TFRS) provides a base model class, `tfrs.models.Model`, that simplifies the process of building models. \n",
        "* All we need to do is set up the necessary components within the `__init__` method and implement the `compute_loss` method, which takes the raw features as input and returns the corresponding loss value.\n",
        "\n",
        "By utilizing the base model class, the framework handles the creation of an appropriate training loop to train our model efficiently and effectively. This streamlines the model-building process and allows us to focus on defining the components and loss computation specific to our recommendation task."
      ],
      "metadata": {
        "id": "Y6xwMp3vlAMX"
      },
      "id": "Y6xwMp3vlAMX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3c8ca3",
      "metadata": {
        "id": "6b3c8ca3"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "    def __init__(self, user_model, movie_model):\n",
        "        super().__init__()\n",
        "        self.movie_model: tf.keras.Model = movie_model\n",
        "        self.user_model: tf.keras.Model = user_model\n",
        "        self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        # We pick out the user features and pass them into the user model.\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        # And pick out the movie features and pass them into the movie model,\n",
        "        # getting embeddings back.\n",
        "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "        # The task computes the loss and the metrics.\n",
        "        return self.task(user_embeddings, positive_movie_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tfrs.Model` base class is a simply convenience class: it allows us to compute both training and test losses using the same method.\n",
        "\n",
        "Under the hood, it's still a plain `Keras` model. "
      ],
      "metadata": {
        "id": "FU73w_gdlpuN"
      },
      "id": "FU73w_gdlpuN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Model Training, Fitting & Evaluation\n"
      ],
      "metadata": {
        "id": "SVy-otj8lgaB"
      },
      "id": "SVy-otj8lgaB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06ac4ecb",
      "metadata": {
        "id": "06ac4ecb"
      },
      "outputs": [],
      "source": [
        "# model instantiation\n",
        "model = MovielensModel(user_model, movie_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a75534",
      "metadata": {
        "id": "18a75534"
      },
      "outputs": [],
      "source": [
        "# shuffle, batch, and cache the training and evaluation data\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b20c57",
      "metadata": {
        "id": "c7b20c57"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "model.fit(cached_train, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497b446d",
      "metadata": {
        "id": "497b446d"
      },
      "outputs": [],
      "source": [
        "# evaluate on the test set\n",
        "model.evaluate(cached_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the test set is considerably worse compared to the training set, and there are two main factors contributing to this discrepancy:\n",
        "\n",
        "1. `Overfitting`: The model tends to perform better on the data it has been exposed to during training because it can memorize it. This issue, known as overfitting, is particularly pronounced when models have a large number of parameters. To address this, we can employ model regularization techniques and leverage user and movie features to enhance the model's ability to generalize to unseen data.\n",
        "\n",
        "2. The model sometimes recommends movies that users have already watched, which can overshadow the recommendations for test movies. These previously watched movies, which are known-positive watches, can dominate the top K recommendations for the test set.\n",
        "\n",
        "One way to mitigate the second phenomenon is to exclude movies that users have already seen from the test recommendations. This approach is commonly employed in recommender systems research, but it is not implemented in this tutorials. \n",
        "\n",
        "If the exclusion of past watches is a crucial requirement, appropriately designed models should be capable of learning this behavior automatically by leveraging historical user data and contextual information. Furthermore, it is often appropriate to recommend the same item multiple times, such as evergreen TV series or regularly purchased items."
      ],
      "metadata": {
        "id": "7VE7t5VdnN3z"
      },
      "id": "7VE7t5VdnN3z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Making Predictions\n",
        "\n",
        "\n",
        "With our model in place, the next step is to utilize it for making predictions. To achieve this, we can employ the `tfrs.layers.factorized_top_k.BruteForce layer`.\n",
        "\n",
        "By integrating the `tfrs.layers.factorized_top_k.BruteForce` layer into our model, we gain the capability to generate predictions. \n",
        "This layer utilizes a brute force approach to efficiently compute the top K recommendations based on the learned embeddings from our model."
      ],
      "metadata": {
        "id": "ZcMwKVZJmb6w"
      },
      "id": "ZcMwKVZJmb6w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac60c30",
      "metadata": {
        "id": "5ac60c30"
      },
      "outputs": [],
      "source": [
        "# Create a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "\n",
        "# recommends movies out of the entire movies dataset.\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
        ")\n",
        "# Get recommendations.\n",
        "_, titles = index(tf.constant([\"42\"]))\n",
        "print(f\"Recommendations for user 42: {titles[0, :5]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}