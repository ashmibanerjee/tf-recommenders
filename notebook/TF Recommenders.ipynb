{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd8c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af9b141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80872f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b33674f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 21:14:35.492104: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d505ecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'You So Crazy (1994)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 16:24:27.387887: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2108ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22cfb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\", 'user_id': b'138'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 21:20:24.435199: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ca5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dece050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b'Postman, The (1997)', 'user_id': b'681'}\n",
      "{'movie_title': b'Clueless (1995)', 'user_id': b'442'}\n",
      "{'movie_title': b'Maltese Falcon, The (1941)', 'user_id': b'932'}\n"
     ]
    }
   ],
   "source": [
    "for x in train.take(3).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc92768e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
       "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
       "       b'2 Days in the Valley (1996)',\n",
       "       b'20,000 Leagues Under the Sea (1954)',\n",
       "       b'2001: A Space Odyssey (1968)',\n",
       "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
       "       b'39 Steps, The (1935)'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_movie_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad72bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61ae15a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aca8cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_user_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f475ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69aabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65848ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "585ced7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13668024",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a15a58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b3c8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.movie_model: tf.keras.Model = movie_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the movie features and pass them into the movie model,\n",
    "        # getting embeddings back.\n",
    "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ac4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a75534",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b20c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 333ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0097 - factorized_top_k/top_10_categorical_accuracy: 0.0205 - factorized_top_k/top_50_categorical_accuracy: 0.1004 - factorized_top_k/top_100_categorical_accuracy: 0.1774 - loss: 69885.1094 - regularization_loss: 0.0000e+00 - total_loss: 69885.1094\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 3s 319ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0028 - factorized_top_k/top_5_categorical_accuracy: 0.0187 - factorized_top_k/top_10_categorical_accuracy: 0.0378 - factorized_top_k/top_50_categorical_accuracy: 0.1696 - factorized_top_k/top_100_categorical_accuracy: 0.2929 - loss: 67523.3693 - regularization_loss: 0.0000e+00 - total_loss: 67523.3693\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 3s 319ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0225 - factorized_top_k/top_10_categorical_accuracy: 0.0459 - factorized_top_k/top_50_categorical_accuracy: 0.1882 - factorized_top_k/top_100_categorical_accuracy: 0.3162 - loss: 66302.9567 - regularization_loss: 0.0000e+00 - total_loss: 66302.9567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2820a3f40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "497b446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 126ms/step - factorized_top_k/top_1_categorical_accuracy: 7.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0098 - factorized_top_k/top_10_categorical_accuracy: 0.0222 - factorized_top_k/top_50_categorical_accuracy: 0.1246 - factorized_top_k/top_100_categorical_accuracy: 0.2325 - loss: 31079.0635 - regularization_loss: 0.0000e+00 - total_loss: 31079.0635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.000750000006519258,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.009800000116229057,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.02215000055730343,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.12460000067949295,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.2325499951839447,\n",
       " 'loss': 28244.771484375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28244.771484375}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ac60c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)'\n",
      " b'101 Dalmatians (1996)' b'While You Were Sleeping (1995)']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index(movies.batch(100).map(model.movie_model), movies)\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0406b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
